{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ade3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, jit, lax, value_and_grad\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from src.neural_network_decoders import TransformerDecoder, print_params_structure\n",
    "from qecsim.models.rotatedplanar import RotatedPlanarCode\n",
    "\n",
    "key = random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf50af",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8ebe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syndromes_initial: (2000, 1000, 8)\n",
      "observables: (2000, 1000)\n",
      "is_using_the_z_basis: (2000,)\n",
      "syndromes_rounds: (2000, 1000, 3, 8)\n",
      "deformations: (2000, 9)\n",
      "\n",
      "x_init: (2000000, 17)\n",
      "x: (2000000, 3, 17)\n",
      "y: (2000000,)\n",
      "bz: (2000000,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data_sets/stim_spin_3x3_r3/\"\n",
    "code = RotatedPlanarCode(3, 3)\n",
    "\n",
    "data = {\n",
    "    file_name.split(\".\")[0]: jnp.load(f\"{data_dir}/{file_name}\") for file_name in os.listdir(data_dir) if file_name.endswith(\".npy\")\n",
    "}\n",
    "for name, val in data.items():\n",
    "    print(f\"{name}: {val.shape}\")\n",
    "\n",
    "num_deformations, num_shots, num_rounds, num_syndromes = data[\"syndromes_rounds\"].shape\n",
    "x_init = jnp.append(\n",
    "    data[\"syndromes_initial\"], \n",
    "    jnp.tile(data[\"deformations\"][:, None, :], (1, num_shots, 1))+2, \n",
    "    axis=-1\n",
    ").reshape(num_deformations*num_shots, -1)\n",
    "x = jnp.append(\n",
    "    data[\"syndromes_rounds\"], \n",
    "    jnp.tile(data[\"deformations\"][:, None, None, :], (1, num_shots, num_rounds, 1))+2, \n",
    "    axis=-1\n",
    ").reshape(num_deformations*num_shots, num_rounds, -1)\n",
    "y = data[\"observables\"].flatten()\n",
    "bz = jnp.tile(data[\"is_using_the_z_basis\"][:, None], (1, num_shots)).flatten()\n",
    "print()\n",
    "print(f\"x_init: {x_init.shape}\")\n",
    "print(f\"x: {x.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "print(f\"bz: {bz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33caa624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_init:\n",
      "[1 1 0 0 1 1 1 0 4 7 5 7 3 6 4 2 6]\n",
      "\n",
      "x:\n",
      "[[1 1 1 0 1 1 1 1 4 7 5 7 3 6 4 2 6]\n",
      " [1 1 1 0 1 1 1 1 4 7 5 7 3 6 4 2 6]\n",
      " [1 1 1 0 1 1 0 1 4 7 5 7 3 6 4 2 6]]\n",
      "\n",
      "y: 0\n",
      "\n",
      "bz: False\n"
     ]
    }
   ],
   "source": [
    "def shuffle_data(key, *sets):\n",
    "    \"\"\"\n",
    "    Shuffle the data along the first axis.\n",
    "    \n",
    "    Args:\n",
    "        key: JAX random key.\n",
    "        *sets: Variable number of arrays to be shuffled in unison along the first axis.\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - new_key: JAX random key after splitting.\n",
    "            - shuffled_data: List of shuffled arrays.\n",
    "    \"\"\"\n",
    "    subkey, key = random.split(key)\n",
    "    perm = random.permutation(subkey, sets[0].shape[0])\n",
    "    shuffled_data = [set[perm] for set in sets]\n",
    "    return key, shuffled_data\n",
    "\n",
    "key, [x_init, x, y, bz] = shuffle_data(key, x_init, x, y, bz)\n",
    "\n",
    "# Show data for first training sample\n",
    "print(\"\\nx_init:\")\n",
    "print(x_init[0])\n",
    "print(\"\\nx:\")\n",
    "print(x[0])\n",
    "print(\"\\ny:\", y[0])\n",
    "print(\"\\nbz:\", bz[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b11a1b",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc2f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "init_key, shuffle_key = random.split(random.key(0), num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4596d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cords_from_code(code: RotatedPlanarCode) -> tuple[list[tuple[float, float]], list[tuple[float, float]]]:\n",
    "    \"\"\"Get the coordinates of the plaquettes and data qubits from a rotated planar code and returns them as two separate lists.\"\"\"\n",
    "    plaquette_coords = code._plaquette_indices\n",
    "    data_qubit_coords = [(x-.5, y-.5) for y in range(code.size[0]) for x in range(code.size[1])]\n",
    "    return plaquette_coords, data_qubit_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7550e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params\n",
      "  embedder\n",
      "    embedder\n",
      "      embedding:\t shape (8, 32)\n",
      "  transformer_first_round\n",
      "    encoder_layers_0\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "    encoder_layers_1\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "  transformer_internal_round\n",
      "    encoder_layers_0\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "    encoder_layers_1\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "  decoder\n",
      "    fc\n",
      "      kernel:\t shape (32, 4)\n",
      "      bias:\t shape (4,)\n"
     ]
    }
   ],
   "source": [
    "plaquette_coords, data_qubit_coords = cords_from_code(code)\n",
    "model = TransformerDecoder(\n",
    "    site_locations=jnp.array(plaquette_coords + data_qubit_coords),\n",
    "    output_features=4,\n",
    "    vocab_size=8,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    d_model=32,\n",
    "    mlp_dim=128,\n",
    "    training=False\n",
    ")\n",
    "model_params = model.init(init_key, x_init[:batch_size], x[:batch_size])  # Initialize model parameters\n",
    "print_params_structure(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fb3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output for first training sample:\n",
      "in:\n",
      " [1 1 0 0 0 0 1 1 7 5 3 5 6 2 3 5 6]\n",
      "[[1 1 0 1 1 0 1 1 7 5 3 5 6 2 3 5 6]\n",
      " [1 1 1 1 1 0 1 1 7 5 3 5 6 2 3 5 6]\n",
      " [0 1 1 0 1 0 1 1 7 5 3 5 6 2 3 5 6]]\n",
      "Out:\n",
      " [3.89%, 6.69%, 83.31%, 6.11%]\n"
     ]
    }
   ],
   "source": [
    "shuffle_key, [x_init, x, y, bz] = shuffle_data(shuffle_key, x_init, x, y, bz)\n",
    "input = (x_init[:1], x[:1])  # Input for a single training sample\n",
    "output = model.apply(model_params, *input)[0]  # Forward pass with a batch of data\n",
    "print(\"\\nModel output for first training sample:\")\n",
    "print(f\"in:\\n {input[0][0]}\\n{input[1][0]}\")\n",
    "print(f\"Out:\\n [{', '.join(f'{p:.2%}' for p in output)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e661c",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f525fa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.884356, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = jnp.clip(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    # Calculate binary cross-entropy\n",
    "    return -jnp.mean(y_true * jnp.log(y_pred) + (1 - y_true) * jnp.log(1 - y_pred))\n",
    "\n",
    "@jit\n",
    "def loss_fn(params, x_init, x, y, bz):\n",
    "    # Get model predictions\n",
    "    p = model.apply(params, x_init, x)\n",
    "    p_I, p_X, p_Y, p_Z = p[:,0], p[:,1], p[:,2], p[:,3]\n",
    "    # Calculate probability of a logical flip based on the basis\n",
    "    p_flip_z = p_X + p_Y\n",
    "    p_flip_x = p_Z + p_Y\n",
    "    # Chose which flip to predict based on the basis used\n",
    "    p_flip = bz * p_flip_z + (1.0 - bz) * p_flip_x\n",
    "    # Calculate binary cross-entropy loss\n",
    "    return binary_cross_entropy(y, p_flip)\n",
    "\n",
    "loss_fn(model_params, x_init[:batch_size], x[:batch_size], y[:batch_size], bz[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960cc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.1875, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def estimate_ler(params, x_init, x, y, bz):\n",
    "    probs = model.apply(params, x_init, x)\n",
    "    # Get predicted logical Pauli error\n",
    "    prediction = probs.argmax(axis=1) # Get the class with the highest probability (0: I, 1: X, 2: Y, 3: Z)\n",
    "    err_I = prediction == 0\n",
    "    err_X = prediction == 1\n",
    "    err_Y = prediction == 2\n",
    "    err_Z = prediction == 3\n",
    "    # Determine if a logical flip has occurred based on the basis\n",
    "    y_pred = (err_X & bz) | (err_Y) | (err_Z & ~bz)\n",
    "    # Compare predictions to true labels and estimate the ler\n",
    "    sucess_rate = jnp.mean(y_pred == y)\n",
    "    ler = 1.0 - sucess_rate\n",
    "    return ler\n",
    "\n",
    "estimate_ler(model_params, x_init[:batch_size], x[:batch_size], y[:batch_size], bz[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df9d9d",
   "metadata": {},
   "source": [
    "# Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d969b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = num_epochs * x.shape[0] // batch_size\n",
    "learning_rate = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=1e-3,\n",
    "    warmup_steps=num_batches * 0.05,\n",
    "    decay_steps=num_batches,\n",
    "    end_value=1e-5\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate)\n",
    ")\n",
    "opt_state = optimizer.init(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b27fb",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b530ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def training_step(\n",
    "    opt_state, \n",
    "    model_params: dict, \n",
    "    x_init: jnp.ndarray, \n",
    "    x: jnp.ndarray, \n",
    "    y: jnp.ndarray, \n",
    "    bz: jnp.ndarray, \n",
    "    batch_idx: int\n",
    ") -> tuple[optax.OptState, dict, float]:\n",
    "    \"\"\"\n",
    "    Perform a single training step: compute loss and gradients, update model parameters.\n",
    "\n",
    "    Args:\n",
    "        opt_state: Current state of the optimizer.\n",
    "        model_params: Current model parameters.\n",
    "        x_init (jnp.ndarray (batch_size, n_sites)): Syndrome measurements at initial round.\n",
    "        x (jnp.ndarray (batch_size, n_rounds, n_sites)): Syndrome measurements for all rounds except the initial round.\n",
    "        y (jnp.ndarray (batch_size,)): Observable outcomes.\n",
    "        bz (jnp.ndarray (batch_size,)): Basis for logical state initialization and measurement (True for Z-basis, False for X-basis).\n",
    "        batch_idx (int): Index of the current batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - new_opt_state: Updated optimizer state.\n",
    "            - new_model_params: Updated model parameters.\n",
    "            - loss: Computed loss for the batch.\n",
    "    \"\"\"\n",
    "    # Get batch data\n",
    "    start = batch_idx * batch_size\n",
    "    batch_x_init = lax.dynamic_slice(x_init, (start, 0), (batch_size, x_init.shape[1]))\n",
    "    batch_x = lax.dynamic_slice(x, (start, 0, 0), (batch_size, x.shape[1], x.shape[2]))\n",
    "    batch_y = lax.dynamic_slice(y, (start,), (batch_size,))\n",
    "    batch_bz = lax.dynamic_slice(bz, (start,), (batch_size,))\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = value_and_grad(loss_fn)(model_params, batch_x_init, batch_x, batch_y, batch_bz)\n",
    "    # Update model parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model_params)\n",
    "    model_params = optax.apply_updates(model_params, updates)\n",
    "    return opt_state, model_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f0933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                                                             | 0/31250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:17<00:00, 404.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.5369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:09<00:00, 449.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.4750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:09<00:00, 447.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Loss: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:09<00:00, 447.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.4549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:09<00:00, 447.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Loss: 0.4503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:10<00:00, 443.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Loss: 0.4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:10<00:00, 440.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Loss: 0.4434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:10<00:00, 441.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:11<00:00, 434.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Loss: 0.4382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31250/31250 [01:10<00:00, 444.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Loss: 0.4368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data at the start of each epoch\n",
    "    shuffle_key, [x_init, x, y, bz] = shuffle_data(shuffle_key, x_init, x, y, bz)\n",
    "\n",
    "    # Training loop\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=200):\n",
    "        opt_state, model_params, loss = training_step(\n",
    "            opt_state, model_params, x_init, x, y, bz, batch_idx\n",
    "        )\n",
    "        epoch_loss += loss\n",
    "    epoch_loss /= num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a756d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jax\n",
    "\n",
    "with open(f\"transformer_model_params.json\", \"w\") as f:\n",
    "    json.dump(jax.tree.map(lambda x: x.tolist(), model_params), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08709ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(43012, dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def params_count(params):\n",
    "    return sum(jnp.prod(jnp.array(p.shape)) for p in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "params_count(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b9f7e",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9471b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated LER on training data: 17.5000%\n"
     ]
    }
   ],
   "source": [
    "num_estimation_samples = 1000\n",
    "shuffle_key, [x_init, x, y, bz] = shuffle_data(shuffle_key, x_init, x, y, bz)\n",
    "estimated_ler = estimate_ler(model_params, x_init[:num_estimation_samples], x[:num_estimation_samples], y[:num_estimation_samples], bz[:num_estimation_samples])\n",
    "print(f\"\\nEstimated LER on training data: {estimated_ler:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37c10d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 0:\n",
      "Experiment in basis Z with observable outcome 0\n",
      "P_I = 41.10%,\n",
      "P_X =  5.29%,\n",
      "P_Y =  6.17%,\n",
      "P_Z = 47.45%\n",
      "\n",
      "Experiment 1:\n",
      "Experiment in basis Z with observable outcome 0\n",
      "P_I = 39.19%,\n",
      "P_X = 29.29%,\n",
      "P_Y = 13.77%,\n",
      "P_Z = 17.75%\n",
      "\n",
      "Experiment 2:\n",
      "Experiment in basis X with observable outcome 0\n",
      "P_I = 33.55%,\n",
      "P_X = 53.44%,\n",
      "P_Y =  7.85%,\n",
      "P_Z =  5.16%\n",
      "\n",
      "Experiment 3:\n",
      "Experiment in basis Z with observable outcome 1\n",
      "P_I = 14.70%,\n",
      "P_X =  6.83%,\n",
      "P_Y = 25.08%,\n",
      "P_Z = 53.39%\n",
      "\n",
      "Experiment 4:\n",
      "Experiment in basis Z with observable outcome 0\n",
      "P_I =  0.75%,\n",
      "P_X =  6.94%,\n",
      "P_Y = 83.43%,\n",
      "P_Z =  8.88%\n",
      "\n",
      "Experiment 5:\n",
      "Experiment in basis X with observable outcome 0\n",
      "P_I = 26.75%,\n",
      "P_X = 32.00%,\n",
      "P_Y = 22.61%,\n",
      "P_Z = 18.65%\n",
      "\n",
      "Experiment 6:\n",
      "Experiment in basis Z with observable outcome 0\n",
      "P_I = 21.56%,\n",
      "P_X =  9.13%,\n",
      "P_Y = 22.17%,\n",
      "P_Z = 47.14%\n",
      "\n",
      "Experiment 7:\n",
      "Experiment in basis X with observable outcome 1\n",
      "P_I =  0.32%,\n",
      "P_X =  4.41%,\n",
      "P_Y = 88.92%,\n",
      "P_Z =  6.36%\n",
      "\n",
      "Experiment 8:\n",
      "Experiment in basis X with observable outcome 0\n",
      "P_I = 16.45%,\n",
      "P_X = 73.72%,\n",
      "P_Y =  8.19%,\n",
      "P_Z =  1.64%\n",
      "\n",
      "Experiment 9:\n",
      "Experiment in basis Z with observable outcome 1\n",
      "P_I =  6.71%,\n",
      "P_X = 54.56%,\n",
      "P_Y = 34.28%,\n",
      "P_Z =  4.44%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"\\nExperiment {i}:\")\n",
    "    print(f\"Experiment in basis {'Z' if bz[i] else 'X'} with observable outcome {y[i]}\")\n",
    "    # print(f\"Input round 0: {x_init[i]}\")\n",
    "    # for r in range(x.shape[1]):\n",
    "    #     print(f\"Input round {r+1}: {x[i, r]}\")\n",
    "    print(\",\\n\".join(f\"P_{pauli} = {p:6.2%}\" for pauli, p in zip(\"IXYZ\", model.apply(model_params, x_init[i:i+1], x[i:i+1])[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
