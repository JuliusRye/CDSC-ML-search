{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ade3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, jit, lax, value_and_grad\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from src.neural_network_decoders import TransformerDecoder, print_params_structure\n",
    "from qecsim.models.rotatedplanar import RotatedPlanarCode\n",
    "\n",
    "key = random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf50af",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8ebe41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deformations: (2000, 9)\n",
      "is_using_the_z_basis: (2000,)\n",
      "observables: (2000, 1000)\n",
      "syndromes_initial: (2000, 1000, 8)\n",
      "syndromes_rounds: (2000, 1000, 3, 8)\n",
      "\n",
      "x_init: (2000000, 17)\n",
      "x: (2000000, 3, 17)\n",
      "y: (2000000,)\n",
      "bz: (2000000,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data_sets/stim_spin_3x3_r3/\"\n",
    "code = RotatedPlanarCode(3, 3)\n",
    "\n",
    "data = {\n",
    "    file_name.split(\".\")[0]: jnp.load(f\"{data_dir}/{file_name}\") for file_name in os.listdir(data_dir) if file_name.endswith(\".npy\")\n",
    "}\n",
    "for name, val in data.items():\n",
    "    print(f\"{name}: {val.shape}\")\n",
    "\n",
    "num_deformations, num_shots, num_rounds, num_syndromes = data[\"syndromes_rounds\"].shape\n",
    "x_init = jnp.append(\n",
    "    data[\"syndromes_initial\"], \n",
    "    jnp.tile(data[\"deformations\"][:, None, :], (1, num_shots, 1))+2, \n",
    "    axis=-1\n",
    ").reshape(num_deformations*num_shots, -1)\n",
    "x = jnp.append(\n",
    "    data[\"syndromes_rounds\"], \n",
    "    jnp.tile(data[\"deformations\"][:, None, None, :], (1, num_shots, num_rounds, 1))+2, \n",
    "    axis=-1\n",
    ").reshape(num_deformations*num_shots, num_rounds, -1)\n",
    "y = data[\"observables\"].flatten()\n",
    "bz = jnp.tile(data[\"is_using_the_z_basis\"][:, None], (1, num_shots)).flatten()\n",
    "print()\n",
    "print(f\"x_init: {x_init.shape}\")\n",
    "print(f\"x: {x.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "print(f\"bz: {bz.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33caa624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x_init:\n",
      "[1 1 0 0 1 1 1 0 4 7 5 7 3 6 4 2 6]\n",
      "\n",
      "x:\n",
      "[[1 1 1 0 1 1 1 1 4 7 5 7 3 6 4 2 6]\n",
      " [1 1 1 0 1 1 1 1 4 7 5 7 3 6 4 2 6]\n",
      " [1 1 1 0 1 1 0 1 4 7 5 7 3 6 4 2 6]]\n",
      "\n",
      "y: 0\n",
      "\n",
      "bz: False\n"
     ]
    }
   ],
   "source": [
    "def shuffle_data(key, *sets):\n",
    "    \"\"\"\n",
    "    Shuffle the data along the first axis.\n",
    "    \n",
    "    Args:\n",
    "        key: JAX random key.\n",
    "        *sets: Variable number of arrays to be shuffled in unison along the first axis.\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - new_key: JAX random key after splitting.\n",
    "            - shuffled_data: List of shuffled arrays.\n",
    "    \"\"\"\n",
    "    subkey, key = random.split(key)\n",
    "    perm = random.permutation(subkey, sets[0].shape[0])\n",
    "    shuffled_data = [set[perm] for set in sets]\n",
    "    return key, shuffled_data\n",
    "\n",
    "key, [x_init, x, y, bz] = shuffle_data(key, x_init, x, y, bz)\n",
    "\n",
    "# Show data for first training sample\n",
    "print(\"\\nx_init:\")\n",
    "print(x_init[0])\n",
    "print(\"\\nx:\")\n",
    "print(x[0])\n",
    "print(\"\\ny:\", y[0])\n",
    "print(\"\\nbz:\", bz[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b11a1b",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfc2f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "init_key, shuffle_key = random.split(random.key(0), num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4596d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cords_from_code(code: RotatedPlanarCode) -> tuple[list[tuple[float, float]], list[tuple[float, float]]]:\n",
    "    \"\"\"Get the coordinates of the plaquettes and data qubits from a rotated planar code and returns them as two separate lists.\"\"\"\n",
    "    plaquette_coords = code._plaquette_indices\n",
    "    data_qubit_coords = [(x-.5, y-.5) for y in range(code.size[0]) for x in range(code.size[1])]\n",
    "    return plaquette_coords, data_qubit_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f7550e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params\n",
      "  embedder\n",
      "    embedder\n",
      "      embedding:\t shape (8, 32)\n",
      "  transformer_first_round\n",
      "    encoder_layers_0\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "    encoder_layers_1\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "  transformer_internal_round\n",
      "    encoder_layers_0\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "    encoder_layers_1\n",
      "      norm_attention\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      attention\n",
      "        query\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        key\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        value\n",
      "          kernel:\t shape (32, 4, 8)\n",
      "          bias:\t shape (4, 8)\n",
      "        out\n",
      "          kernel:\t shape (4, 8, 32)\n",
      "          bias:\t shape (32,)\n",
      "      norm_mlp\n",
      "        scale:\t shape (32,)\n",
      "        bias:\t shape (32,)\n",
      "      gated_mlp\n",
      "        fc_layer_0\n",
      "          kernel:\t shape (32, 128)\n",
      "          bias:\t shape (128,)\n",
      "        fc_layer_1\n",
      "          kernel:\t shape (64, 32)\n",
      "          bias:\t shape (32,)\n",
      "  decoder\n",
      "    fc\n",
      "      kernel:\t shape (32, 4)\n",
      "      bias:\t shape (4,)\n"
     ]
    }
   ],
   "source": [
    "plaquette_coords, data_qubit_coords = cords_from_code(code)\n",
    "model = TransformerDecoder(\n",
    "    site_locations=jnp.array(plaquette_coords + data_qubit_coords),\n",
    "    output_features=4,\n",
    "    vocab_size=8,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    d_model=32,\n",
    "    mlp_dim=128,\n",
    "    training=False\n",
    ")\n",
    "model_params = model.init(init_key, x_init[:batch_size], x[:batch_size])  # Initialize model parameters\n",
    "print_params_structure(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "17fb3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output for first training sample:\n",
      "in:\n",
      " [0 1 0 0 0 0 1 1 5 2 5 3 2 7 4 4 2]\n",
      "[[0 1 0 0 0 0 1 1 5 2 5 3 2 7 4 4 2]\n",
      " [0 1 0 0 0 0 1 1 5 2 5 3 2 7 4 4 2]\n",
      " [0 1 0 0 0 0 1 1 5 2 5 3 2 7 4 4 2]]\n",
      "Out:\n",
      " [0.76%, 2.36%, 54.59%, 42.28%]\n"
     ]
    }
   ],
   "source": [
    "shuffle_key, [x_init, x, y, bz] = shuffle_data(shuffle_key, x_init, x, y, bz)\n",
    "input = (x_init[:1], x[:1])  # Input for a single training sample\n",
    "output = model.apply(model_params, *input)[0]  # Forward pass with a batch of data\n",
    "print(\"\\nModel output for first training sample:\")\n",
    "print(f\"in:\\n {input[0][0]}\\n{input[1][0]}\")\n",
    "print(f\"Out:\\n [{', '.join(f'{p:.2%}' for p in output)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e661c",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f525fa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.0337628, dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = jnp.clip(y_pred, 1e-7, 1.0 - 1e-7)\n",
    "    # Calculate binary cross-entropy\n",
    "    return -jnp.mean(y_true * jnp.log(y_pred) + (1 - y_true) * jnp.log(1 - y_pred))\n",
    "\n",
    "@jit\n",
    "def loss_fn(params, x_init, x, y, bz):\n",
    "    # Get model predictions\n",
    "    p = model.apply(params, x_init, x)\n",
    "    p_I, p_X, p_Y, p_Z = p[:,0], p[:,1], p[:,2], p[:,3]\n",
    "    # Calculate probability of a logical flip based on the basis\n",
    "    p_flip_z = p_X + p_Y\n",
    "    p_flip_x = p_Z + p_Y\n",
    "    # Chose which flip to predict based on the basis used\n",
    "    p_flip = bz * p_flip_z + (1.0 - bz) * p_flip_x\n",
    "    # Calculate binary cross-entropy loss\n",
    "    return binary_cross_entropy(y, p_flip)\n",
    "\n",
    "loss_fn(model_params, x_init[:batch_size], x[:batch_size], y[:batch_size], bz[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0960cc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.5, dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def estimate_ler(params, x_init, x, y, bz):\n",
    "    probs = model.apply(params, x_init, x)\n",
    "    # Get predicted logical Pauli error\n",
    "    prediction = probs.argmax(axis=1) # Get the class with the highest probability (0: I, 1: X, 2: Y, 3: Z)\n",
    "    err_I = prediction == 0\n",
    "    err_X = prediction == 1\n",
    "    err_Y = prediction == 2\n",
    "    err_Z = prediction == 3\n",
    "    # Determine if a logical flip has occurred based on the basis\n",
    "    y_pred = (err_X & bz) | (err_Y) | (err_Z & ~bz)\n",
    "    # Compare predictions to true labels and estimate the ler\n",
    "    sucess_rate = jnp.mean(y_pred == y)\n",
    "    ler = 1.0 - sucess_rate\n",
    "    return ler\n",
    "\n",
    "estimate_ler(model_params, x_init[:batch_size], x[:batch_size], y[:batch_size], bz[:batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df9d9d",
   "metadata": {},
   "source": [
    "# Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f1d969b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = num_epochs * x.shape[0] // batch_size\n",
    "learning_rate = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=0.0,\n",
    "    peak_value=1e-3,\n",
    "    warmup_steps=num_batches * 0.05,\n",
    "    decay_steps=num_batches,\n",
    "    end_value=1e-5\n",
    ")\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adamw(learning_rate)\n",
    ")\n",
    "opt_state = optimizer.init(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b27fb",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "04b530ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def training_step(\n",
    "    opt_state, \n",
    "    model_params: dict, \n",
    "    x_init: jnp.ndarray, \n",
    "    x: jnp.ndarray, \n",
    "    y: jnp.ndarray, \n",
    "    bz: jnp.ndarray, \n",
    "    batch_idx: int\n",
    ") -> tuple[optax.OptState, dict, float]:\n",
    "    \"\"\"\n",
    "    Perform a single training step: compute loss and gradients, update model parameters.\n",
    "\n",
    "    Args:\n",
    "        opt_state: Current state of the optimizer.\n",
    "        model_params: Current model parameters.\n",
    "        x_init (jnp.ndarray (batch_size, n_sites)): Syndrome measurements at initial round.\n",
    "        x (jnp.ndarray (batch_size, n_rounds, n_sites)): Syndrome measurements for all rounds except the initial round.\n",
    "        y (jnp.ndarray (batch_size,)): Observable outcomes.\n",
    "        bz (jnp.ndarray (batch_size,)): Basis for logical state initialization and measurement (True for Z-basis, False for X-basis).\n",
    "        batch_idx (int): Index of the current batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - new_opt_state: Updated optimizer state.\n",
    "            - new_model_params: Updated model parameters.\n",
    "            - loss: Computed loss for the batch.\n",
    "    \"\"\"\n",
    "    # Get batch data\n",
    "    start = batch_idx * batch_size\n",
    "    batch_x_init = lax.dynamic_slice(x_init, (start, 0), (batch_size, x_init.shape[1]))\n",
    "    batch_x = lax.dynamic_slice(x, (start, 0, 0), (batch_size, x.shape[1], x.shape[2]))\n",
    "    batch_y = lax.dynamic_slice(y, (start,), (batch_size,))\n",
    "    batch_bz = lax.dynamic_slice(bz, (start,), (batch_size,))\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = value_and_grad(loss_fn)(model_params, batch_x_init, batch_x, batch_y, batch_bz)\n",
    "    # Update model parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, model_params)\n",
    "    model_params = optax.apply_updates(model_params, updates)\n",
    "    return opt_state, model_params, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c6f0933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                                                             | 0/31250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   4%|██████                                                                                                                                            | 1306/31250 [01:31<34:53, 14.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m epoch_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_batches), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, ncols=\u001b[32m200\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     opt_state, model_params, loss = \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     epoch_loss += loss\n\u001b[32m     13\u001b[39m epoch_loss /= num_batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(_cls)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data at the start of each epoch\n",
    "    shuffle_key, [x_init, x, y, bz] = shuffle_data(shuffle_key, x_init, x, y, bz)\n",
    "\n",
    "    # Training loop\n",
    "    num_batches = x.shape[0] // batch_size\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=200):\n",
    "        opt_state, model_params, loss = training_step(\n",
    "            opt_state, model_params, x_init, x, y, bz, batch_idx\n",
    "        )\n",
    "        epoch_loss += loss\n",
    "    epoch_loss /= num_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
